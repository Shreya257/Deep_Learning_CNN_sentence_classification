One of the earliest applications of CNN in Natural Language Processing was introduced in the paper Convolutional Neural Networks for Sentence Classification (Kim, 2014). 
Same idea as in computer vision, CNN model is used as a feature extractor that encodes semantic features of sentences before these features are fed to a classifier. 
We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. 
We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. 
Learning task-specific vectors through fine-tuning offers further gains in performance. 
We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. 
We are building and training CNN model with PyTorch. 
Our results show that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.
